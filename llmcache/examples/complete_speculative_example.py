"""å®Œæ•´çš„æŠ•æœºè§£ç é…ç½®æ¡ˆä¾‹

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨llmcacheé¡¹ç›®ä¸­ä½¿ç”¨æŠ•æœºè§£ç é…ç½®æ¥æå‡æ¨ç†æ€§èƒ½ã€‚
åŒ…å«ï¼š
1. å®Œæ•´çš„é…ç½®è®¾ç½®
2. ç³»ç»Ÿåˆå§‹åŒ–
3. æ€§èƒ½æµ‹è¯•
4. é”™è¯¯å¤„ç†
5. èµ„æºæ¸…ç†
"""
import os
os.environ["VLLM_USE_V1"] = "1"
os.environ['TRANSFORMERS_CACHE'] = '/home/xudebin'
os.environ['TORCH_CUDA_ARCH_LIST'] = '8.0'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import asyncio
import sys
import time
import json
import uuid
from typing import Dict, Any, List, Optional
from pathlib import Path

# Add the parent directory to Python path to import from src
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from vllm.sampling_params import SamplingParams
from vllm.config import VllmConfig
from vllm.v1.executor.abstract import Executor

# å¯¼å…¥å¢å¼ºç»„ä»¶
from src.handler.enhanced_async_llm import EnhancedAsyncLLM
from src.cache.multi_level_cache import MultiLevelCache
from src.utils.similarity_search_helper import SimilaritySearchHelper
from src.config.settings import CacheConfig, VectorSearchConfig
from src.config.handler_config import HandlerConfig, HandlerFeatureConfig
from src.models.enums import CacheLevel


class CompleteSpeculativeDemo:
    """å®Œæ•´çš„æŠ•æœºè§£ç æ¼”ç¤ºæ¡ˆä¾‹"""
    
    def __init__(self):
        self.enhanced_llm: Optional[EnhancedAsyncLLM] = None
        self.cache_manager: Optional[MultiLevelCache] = None
        self.similarity_search_helper: Optional[SimilaritySearchHelper] = None
        self.performance_stats = []
    
    def create_speculative_config(self) -> Dict[str, Any]:
        """åˆ›å»ºæŠ•æœºè§£ç é…ç½®
        
        Returns:
            æŠ•æœºè§£ç é…ç½®å­—å…¸
        """
        return {
            "method": "myngram",
            "num_speculative_tokens": 100,
            "prompt_lookup_max": 5,
            "prompt_lookup_min": 1,
        }
    
    def create_vllm_config(self) -> VllmConfig:
        """åˆ›å»ºVLLMé…ç½®
        
        Returns:
            é…ç½®å¥½çš„VllmConfigå®ä¾‹
        """
        from vllm.engine.arg_utils import AsyncEngineArgs
        
        # æŠ•æœºè§£ç é…ç½®
        speculative_config = self.create_speculative_config()
        
        # åˆ›å»ºAsyncEngineArgså¹¶è®¾ç½®æŠ•æœºè§£ç 
        engine_args = AsyncEngineArgs(
            model="Qwen/Qwen2.5-7B-Instruct",
            dtype="bfloat16",
            gpu_memory_utilization=0.90,
            speculative_config=speculative_config
        )
        
        # åˆ›å»ºVllmConfig
        return engine_args.create_engine_config()
    
    def create_cache_config(self) -> CacheConfig:
        """åˆ›å»ºç¼“å­˜é…ç½®"""
        return CacheConfig(
            enable_redis=True,
            redis_host="localhost",
            redis_port=6379,
            redis_db=0,
            redis_password=None,
            max_memory_entries=2000,
            memory_ttl=7200,  # 2å°æ—¶
            redis_ttl=7200
        )
    
    def create_similarity_config(self) -> VectorSearchConfig:
        """åˆ›å»ºç›¸ä¼¼åº¦æœç´¢é…ç½®"""
        return VectorSearchConfig(
            enabled=True,
            similarity_threshold=0.7,
            max_results=10,
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            use_gpu=True
        )
    
    def create_handler_config(self) -> HandlerConfig:
        """åˆ›å»ºå¤„ç†å™¨é…ç½®"""
        config = HandlerConfig()
        
        # åŠŸèƒ½é…ç½®
        config.features = HandlerFeatureConfig(
            enable_nlp_enhancement=True,
            enable_similarity_search=True,
            enable_caching=True,
            enable_detailed_logging=True,
            enable_vector_search=True
            # ç§»é™¤äº† enable_async_processing=True è¿™ä¸ªä¸å­˜åœ¨çš„å‚æ•°
        )
        
        # æ€§èƒ½é…ç½®
        config.max_concurrent_tasks = 50
        config.request_timeout = 300  # 5åˆ†é’Ÿ
        config.batch_size = 16
        config.enable_streaming = True
        
        return config
    
    async def setup_system(self) -> None:
        """è®¾ç½®å®Œæ•´çš„å¢å¼ºç³»ç»Ÿ"""
        try:
            logger.info("ğŸš€ å¼€å§‹è®¾ç½®å®Œæ•´çš„æŠ•æœºè§£ç å¢å¼ºç³»ç»Ÿ...")
            
            # åˆ›å»ºæ‰€æœ‰é…ç½®
            vllm_config = self.create_vllm_config()
            cache_config = self.create_cache_config()
            similarity_config = self.create_similarity_config()
            handler_config = self.create_handler_config()
            
            # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
            speculative_config = self.create_speculative_config()
            logger.info(f"ğŸ“‹ æŠ•æœºè§£ç é…ç½®: {json.dumps(speculative_config, indent=2)}")
            logger.info(f"ğŸ¯ æ¨¡å‹: {vllm_config.model_config.model}")
            logger.info(f"ğŸ’¾ GPUå†…å­˜åˆ©ç”¨ç‡: {vllm_config.cache_config.gpu_memory_utilization}")
            logger.info(f"ğŸ”„ ç¼“å­˜å¯ç”¨: {cache_config.enable_redis}")
            logger.info(f"ğŸ” ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_config.similarity_threshold}")

            self.cache_manager = MultiLevelCache()
        
            # åˆ›å»ºç›¸ä¼¼åº¦æœç´¢åŠ©æ‰‹
            self.similarity_search_helper = SimilaritySearchHelper()
            self.similarity_search_helper.initialize(similarity_config)
            # ä½¿ç”¨æ–°çš„from_vllm_configç±»æ–¹æ³•åˆ›å»ºå¢å¼ºçš„AsyncLLM
            self.enhanced_llm = EnhancedAsyncLLM.from_vllm_config(
                vllm_config=vllm_config,
                cache_manager=self.cache_manager,
                similarity_search_helper=self.similarity_search_helper,
                handler_config=handler_config,
                start_engine_loop=True,
                nlp_max_concurrent=30
            )
            
            logger.info("âœ… å®Œæ•´çš„æŠ•æœºè§£ç å¢å¼ºç³»ç»Ÿè®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿè®¾ç½®å¤±è´¥: {e}")
            raise

    async def run_performance_test(self, test_name: str, prompts: List[str], 
                                 sampling_params: SamplingParams) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•
        
        Args:
            test_name: æµ‹è¯•åç§°
            prompts: æµ‹è¯•æç¤ºåˆ—è¡¨
            sampling_params: é‡‡æ ·å‚æ•°
            
        Returns:
            æ€§èƒ½ç»Ÿè®¡ç»“æœ
        """
        logger.info(f"ğŸ§ª å¼€å§‹æ€§èƒ½æµ‹è¯•: {test_name}")
        
        start_time = time.time()
        total_tokens = 0
        cache_hits = 0
        similar_contexts = 0
        
        for i, prompt in enumerate(prompts, 1):
            logger.info(f"  ğŸ“ å¤„ç†ç¬¬ {i}/{len(prompts)} ä¸ªè¯·æ±‚...")
            
            request_start = time.time()
            
            try:
                # ä½¿ç”¨EnhancedAsyncLLMçš„generate_enhancedæ–¹æ³•
                request_id = str(uuid.uuid4())
                
                async for output in self.enhanced_llm.generate_enhanced(
                    prompt=prompt,
                    sampling_params=sampling_params,
                    request_id=request_id
                ):
                    if output.finished:
                        request_time = time.time() - request_start
                        
                        # è®¡ç®—tokenæ•°é‡
                        token_count = 0
                        output_text = ""
                        if output.outputs:
                            for comp in output.outputs:
                                token_count += len(comp.token_ids) if comp.token_ids else len(comp.text.split())
                                output_text += comp.text
                        
                        total_tokens += token_count
                        
                        # æ£€æŸ¥æ˜¯å¦ä¸ºç¼“å­˜å‘½ä¸­ï¼ˆé€šè¿‡è¾“å‡ºæ—¶é—´åˆ¤æ–­ï¼‰
                        if request_time < 0.1:  # å°äº100msè®¤ä¸ºæ˜¯ç¼“å­˜å‘½ä¸­
                            cache_hits += 1
                        logger.info(f"    â±ï¸  è€—æ—¶: {request_time:.2f}s, tokens: {token_count}")
                        logger.info(f"    ğŸ“ é—®é¢˜: {prompt}")
                        logger.info(f"    ğŸ’¬ ç­”æ¡ˆ: {output_text}")
                        logger.info(f"    {'='*60}")
                        break
                        
            except Exception as e:
                logger.error(f"    âŒ è¯·æ±‚å¤±è´¥: {e}")
                continue
            
            # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡è½½
            await asyncio.sleep(0.2)
        
        total_time = time.time() - start_time
        avg_speed = total_tokens / total_time if total_time > 0 else 0
        
        stats = {
            'test_name': test_name,
            'total_requests': len(prompts),
            'total_time': total_time,
            'total_tokens': total_tokens,
            'avg_speed': avg_speed,
            'cache_hits': cache_hits,
            'cache_hit_rate': cache_hits / len(prompts) if prompts else 0,
            'similar_contexts': similar_contexts,
            'avg_similar_contexts': similar_contexts / len(prompts) if prompts else 0
        }
        
        self.performance_stats.append(stats)
        
        logger.info(f"ğŸ“Š {test_name} å®Œæˆ:")
        logger.info(f"    ğŸš€ å¹³å‡é€Ÿåº¦: {avg_speed:.2f} tokens/s")
        logger.info(f"    ğŸ’¾ ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}")
        logger.info(f"    ğŸ” å¹³å‡ç›¸ä¼¼ä¸Šä¸‹æ–‡: {stats['avg_similar_contexts']:.1f}")
        
        return stats
    
    async def demo_basic_generation(self) -> None:
        """æ¼”ç¤ºåŸºæœ¬ç”ŸæˆåŠŸèƒ½"""
        logger.info("\n=== ğŸ¯ åŸºæœ¬ç”Ÿæˆæ¼”ç¤º ===")
        
        prompts = [
            "Please explain what artificial intelligence is?",
            "What is machine learning?",
            "What are the basic concepts of deep learning?"
        ]
        
        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=200
        )
        
        await self.run_performance_test("åŸºæœ¬ç”Ÿæˆ", prompts, sampling_params)
    
    async def demo_cache_performance(self) -> None:
        """æ¼”ç¤ºç¼“å­˜æ€§èƒ½"""
        logger.info("\n=== ğŸ’¾ ç¼“å­˜æ€§èƒ½æ¼”ç¤º ===")
        
        # é‡å¤ç›¸åŒçš„é—®é¢˜æ¥æµ‹è¯•ç¼“å­˜
        prompts = [
            "What is natural language processing?",
            "What is natural language processing?",  # é‡å¤
            "Explain the basic concepts of computer vision.",
            "Explain the basic concepts of computer vision.",  # é‡å¤
            "What is natural language processing?",  # å†æ¬¡é‡å¤
        ]
        
        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=150
        )
        
        await self.run_performance_test("ç¼“å­˜æ€§èƒ½", prompts, sampling_params)
    
    async def demo_complex_generation(self) -> None:
        """æ¼”ç¤ºå¤æ‚ç”Ÿæˆä»»åŠ¡"""
        logger.info("\n=== ğŸ§  å¤æ‚ç”Ÿæˆæ¼”ç¤º ===")
        
        prompts = [
            "Please explain in detail the differences between deep learning and machine learning, and describe their respective application scenarios and advantages/disadvantages.",
            "Analyze the current application status of artificial intelligence in healthcare, finance, and education, as well as future development trends.",
            "Compare different neural network architectures (CNN, RNN, Transformer) and explain their working principles."
        ]
        
        sampling_params = SamplingParams(
            temperature=0.8,
            top_p=0.95,
            max_tokens=500  # æ›´é•¿çš„è¾“å‡ºä»¥ä½“ç°æŠ•æœºè§£ç ä¼˜åŠ¿
        )
        
        await self.run_performance_test("å¤æ‚ç”Ÿæˆ", prompts, sampling_params)
    
    async def demo_batch_processing(self) -> None:
        """æ¼”ç¤ºæ‰¹é‡å¤„ç†"""
        logger.info("\n=== ğŸ“¦ æ‰¹é‡å¤„ç†æ¼”ç¤º ===")
        
        prompts = [
            "What is reinforcement learning?",
            "Explain the backpropagation algorithm in neural networks.",
            "What is the attention mechanism?",
            "Explain how Generative Adversarial Networks (GANs) work.",
            "What is transfer learning?",
            "Explain the basic structure of convolutional neural networks.",
            "What are recurrent neural networks?",
            "Explain the innovations of the Transformer architecture.",
            "Explain how Generative Adversarial Networks (GANs)."
        ]
        
        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=250
        )
        
        await self.run_performance_test("æ‰¹é‡å¤„ç†", prompts, sampling_params)
    
    async def demo_health_check(self) -> None:
        """æ¼”ç¤ºç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        logger.info("\n=== ğŸ¥ ç³»ç»Ÿå¥åº·æ£€æŸ¥ ===")
        
        try:
            # æ£€æŸ¥å„ç»„ä»¶çŠ¶æ€
            logger.info("ğŸ“‹ ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š:")
            
            # æ£€æŸ¥ç¼“å­˜ç®¡ç†å™¨
            if self.cache_manager:
                cache_stats = await self.cache_manager.get_stats()
                logger.info(f"  ğŸŸ¢ ç¼“å­˜ç®¡ç†å™¨: æ­£å¸¸è¿è¡Œ")
                logger.info(f"    ğŸ“Š ç¼“å­˜ç»Ÿè®¡: {cache_stats}")
            else:
                logger.info("  ğŸ”´ ç¼“å­˜ç®¡ç†å™¨: æœªåˆå§‹åŒ–")
            
            # æ£€æŸ¥ç›¸ä¼¼åº¦æœç´¢åŠ©æ‰‹
            if self.similarity_search_helper:
                logger.info(f"  ğŸŸ¢ ç›¸ä¼¼åº¦æœç´¢: æ­£å¸¸è¿è¡Œ")
            else:
                logger.info("  ğŸ”´ ç›¸ä¼¼åº¦æœç´¢: æœªåˆå§‹åŒ–")
            
            # æ£€æŸ¥å¢å¼ºLLM
            if self.enhanced_llm:
                logger.info(f"  ğŸŸ¢ å¢å¼ºLLM: æ­£å¸¸è¿è¡Œ")
                logger.info(f"    ğŸ”§ æ´»è·ƒä»»åŠ¡æ•°: {len(self.enhanced_llm._active_tasks)}")
            else:
                logger.info("  ğŸ”´ å¢å¼ºLLM: æœªåˆå§‹åŒ–")
            
        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
    
    def generate_performance_report(self) -> None:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info("\n=== ğŸ“Š æ€§èƒ½æŠ¥å‘Š ===")
        
        if not self.performance_stats:
            logger.warning("âš ï¸  æ²¡æœ‰æ€§èƒ½æ•°æ®")
            return
        
        logger.info("ğŸ“ˆ è¯¦ç»†æ€§èƒ½ç»Ÿè®¡:")
        logger.info("-" * 80)
        logger.info(f"{'æµ‹è¯•åç§°':<15} {'è¯·æ±‚æ•°':<8} {'æ€»è€—æ—¶':<10} {'å¹³å‡é€Ÿåº¦':<12} {'ç¼“å­˜å‘½ä¸­ç‡':<12} {'ç›¸ä¼¼ä¸Šä¸‹æ–‡':<12}")
        logger.info("-" * 80)
        
        total_requests = 0
        total_time = 0
        total_tokens = 0
        total_cache_hits = 0
        
        for stats in self.performance_stats:
            logger.info(
                f"{stats['test_name']:<15} "
                f"{stats['total_requests']:<8} "
                f"{stats['total_time']:<10.2f} "
                f"{stats['avg_speed']:<12.2f} "
                f"{stats['cache_hit_rate']:<12.1%} "
                f"{stats['avg_similar_contexts']:<12.1f}"
            )
            
            total_requests += stats['total_requests']
            total_time += stats['total_time']
            total_tokens += stats['total_tokens']
            total_cache_hits += stats['cache_hits']
        
        logger.info("-" * 80)
        
        overall_speed = total_tokens / total_time if total_time > 0 else 0
        overall_cache_rate = total_cache_hits / total_requests if total_requests > 0 else 0
        
        logger.info("ğŸ¯ æ€»ä½“æ€§èƒ½:")
        logger.info(f"  ğŸ“ æ€»è¯·æ±‚æ•°: {total_requests}")
        logger.info(f"  â±ï¸  æ€»è€—æ—¶: {total_time:.2f}s")
        logger.info(f"  ğŸš€ æ€»ä½“å¹³å‡é€Ÿåº¦: {overall_speed:.2f} tokens/s")
        logger.info(f"  ğŸ’¾ æ€»ä½“ç¼“å­˜å‘½ä¸­ç‡: {overall_cache_rate:.1%}")
        logger.info(f"  ğŸ“Š æ€»ç”Ÿæˆtokens: {total_tokens}")
    
    async def cleanup(self) -> None:
        """æ¸…ç†ç³»ç»Ÿèµ„æº"""
        logger.info("\n=== ğŸ§¹ æ¸…ç†ç³»ç»Ÿèµ„æº ===")
        
        try:
            if self.enhanced_llm:
                logger.info("ğŸ›‘ åœæ­¢å¢å¼ºLLM...")
                await self.enhanced_llm.cleanup()
                self.enhanced_llm.shutdown()
            
            if self.cache_manager:
                logger.info("ğŸ’¾ æ¸…ç†ç¼“å­˜ç®¡ç†å™¨...")
                await self.cache_manager.clear()
            
            if self.similarity_search_helper:
                logger.info("ğŸ” æ¸…ç†ç›¸ä¼¼åº¦æœç´¢åŠ©æ‰‹...")
                await self.similarity_search_helper.cleanup()
            
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†èµ„æºå¤±è´¥: {e}")
    
    async def run_complete_demo(self) -> None:
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        try:
            # è®¾ç½®ç³»ç»Ÿ
            await self.setup_system()
            
            # è¿è¡Œå„ç§æ¼”ç¤º
            await self.demo_basic_generation()
            await asyncio.sleep(1)
            
            await self.demo_cache_performance()
            await asyncio.sleep(1)
            
            await self.demo_complex_generation()
            await asyncio.sleep(1)
            
            await self.demo_batch_processing()
            await asyncio.sleep(1)
            
            await self.demo_health_check()
            
            # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
            self.generate_performance_report()
            
        except Exception as e:
            logger.error(f"âŒ æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
            raise
        finally:
            # ç¡®ä¿æ¸…ç†èµ„æº
            await self.cleanup()


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¬ å¼€å§‹å®Œæ•´çš„æŠ•æœºè§£ç é…ç½®æ¼”ç¤º")
    logger.info("=" * 60)
    
    demo = CompleteSpeculativeDemo()
    
    try:
        await demo.run_complete_demo()
        logger.info("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ¼”ç¤º")
        await demo.cleanup()
        
    except Exception as e:
        logger.error(f"\nğŸ’¥ æ¼”ç¤ºå¤±è´¥: {e}")
        await demo.cleanup()
        raise


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        lambda msg: print(msg, end=""),
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
        level="INFO"
    )
    
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())