#!/usr/bin/env python3
"""GPUå·¥å…·æ¨¡å— - æä¾›GPUæ£€æµ‹ã€é…ç½®å’Œä¼˜åŒ–åŠŸèƒ½"""

import torch
import os
from typing import Dict, List, Optional, Tuple
from loguru import logger


class GPUManager:
    """GPUç®¡ç†å™¨ - è´Ÿè´£GPUæ£€æµ‹ã€é…ç½®å’Œä¼˜åŒ–"""
    
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.gpu_count = torch.cuda.device_count() if self.gpu_available else 0
        self.current_device = None
        self.device_info = {}
        
        if self.gpu_available:
            self._initialize_gpu_info()
    
    def _initialize_gpu_info(self):
        """åˆå§‹åŒ–GPUä¿¡æ¯"""
        try:
            for i in range(self.gpu_count):
                device_props = torch.cuda.get_device_properties(i)
                self.device_info[i] = {
                    'name': device_props.name,
                    'total_memory': device_props.total_memory,
                    'major': device_props.major,
                    'minor': device_props.minor,
                    'multi_processor_count': device_props.multi_processor_count
                }
            
            self.current_device = torch.cuda.current_device()
            logger.info(f"GPUåˆå§‹åŒ–å®Œæˆï¼Œå½“å‰è®¾å¤‡: {self.current_device}")
            
        except Exception as e:
            logger.error(f"GPUä¿¡æ¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def get_gpu_status(self) -> Dict:
        """è·å–GPUçŠ¶æ€ä¿¡æ¯"""
        status = {
            'gpu_available': self.gpu_available,
            'gpu_count': self.gpu_count,
            'current_device': self.current_device,
            'pytorch_version': torch.__version__,
            'cuda_version': torch.version.cuda if self.gpu_available else None,
            'devices': []
        }
        
        if self.gpu_available:
            for i in range(self.gpu_count):
                try:
                    memory_allocated = torch.cuda.memory_allocated(i)
                    memory_reserved = torch.cuda.memory_reserved(i)
                    total_memory = self.device_info[i]['total_memory']
                    
                    device_status = {
                        'device_id': i,
                        'name': self.device_info[i]['name'],
                        'compute_capability': f"{self.device_info[i]['major']}.{self.device_info[i]['minor']}",
                        'total_memory_gb': total_memory / (1024**3),
                        'allocated_memory_gb': memory_allocated / (1024**3),
                        'reserved_memory_gb': memory_reserved / (1024**3),
                        'free_memory_gb': (total_memory - memory_reserved) / (1024**3),
                        'utilization_percent': (memory_allocated / total_memory) * 100,
                        'multi_processor_count': self.device_info[i]['multi_processor_count']
                    }
                    status['devices'].append(device_status)
                    
                except Exception as e:
                    logger.warning(f"è·å–è®¾å¤‡ {i} çŠ¶æ€å¤±è´¥: {e}")
        
        return status
    
    def print_gpu_status(self):
        """æ‰“å°GPUçŠ¶æ€ä¿¡æ¯"""
        status = self.get_gpu_status()
        
        print("\n" + "="*60)
        print("ğŸ”§ GPU çŠ¶æ€æ£€æµ‹")
        print("="*60)
        print(f"PyTorchç‰ˆæœ¬: {status['pytorch_version']}")
        print(f"CUDAå¯ç”¨: {status['gpu_available']}")
        
        if status['gpu_available']:
            print(f"CUDAç‰ˆæœ¬: {status['cuda_version']}")
            print(f"GPUæ•°é‡: {status['gpu_count']}")
            print(f"å½“å‰è®¾å¤‡: {status['current_device']}")
            
            for device in status['devices']:
                print(f"\nğŸ“± è®¾å¤‡ {device['device_id']}: {device['name']}")
                print(f"   è®¡ç®—èƒ½åŠ›: {device['compute_capability']}")
                print(f"   æ€»å†…å­˜: {device['total_memory_gb']:.2f} GB")
                print(f"   å·²åˆ†é…: {device['allocated_memory_gb']:.2f} GB")
                print(f"   å·²ä¿ç•™: {device['reserved_memory_gb']:.2f} GB")
                print(f"   å¯ç”¨å†…å­˜: {device['free_memory_gb']:.2f} GB")
                print(f"   åˆ©ç”¨ç‡: {device['utilization_percent']:.1f}%")
                print(f"   å¤šå¤„ç†å™¨æ•°é‡: {device['multi_processor_count']}")
        else:
            print("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPU")
        
        print("="*60)
    
    def get_optimal_device(self) -> Optional[int]:
        """è·å–æœ€ä¼˜çš„GPUè®¾å¤‡ID"""
        if not self.gpu_available:
            return None
        
        status = self.get_gpu_status()
        best_device = None
        max_free_memory = 0
        
        for device in status['devices']:
            if device['free_memory_gb'] > max_free_memory:
                max_free_memory = device['free_memory_gb']
                best_device = device['device_id']
        
        return best_device
    
    def set_device(self, device_id: int) -> bool:
        """è®¾ç½®å½“å‰GPUè®¾å¤‡"""
        if not self.gpu_available or device_id >= self.gpu_count:
            return False
        
        try:
            torch.cuda.set_device(device_id)
            self.current_device = device_id
            logger.info(f"å·²åˆ‡æ¢åˆ°GPUè®¾å¤‡ {device_id}")
            return True
        except Exception as e:
            logger.error(f"åˆ‡æ¢GPUè®¾å¤‡å¤±è´¥: {e}")
            return False
    
    def optimize_for_benepar(self) -> Dict[str, str]:
        """ä¸ºbeneparæ¨¡å‹ä¼˜åŒ–GPUé…ç½®"""
        recommendations = {
            'status': 'success',
            'recommendations': [],
            'warnings': []
        }
        
        if not self.gpu_available:
            recommendations['status'] = 'warning'
            recommendations['warnings'].append(
                "æœªæ£€æµ‹åˆ°GPUï¼Œbenepar_en3_largeåœ¨CPUä¸Šè¿è¡Œä¼šæ¯”æ ‡å‡†ç‰ˆæœ¬æ…¢çº¦3å€"
            )
            recommendations['recommendations'].extend([
                "è€ƒè™‘ä½¿ç”¨benepar_en3æ ‡å‡†ç‰ˆæœ¬ä»¥æé«˜CPUæ€§èƒ½",
                "å¦‚æœå¯èƒ½ï¼Œè¯·é…ç½®GPUç¯å¢ƒä»¥è·å¾—æœ€ä½³æ€§èƒ½"
            ])
            return recommendations
        
        status = self.get_gpu_status()
        best_device = self.get_optimal_device()
        
        if best_device is not None:
            device_info = status['devices'][best_device]
            
            # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
            if device_info['free_memory_gb'] < 2.0:
                recommendations['warnings'].append(
                    f"GPU {best_device} å¯ç”¨å†…å­˜è¾ƒå°‘ ({device_info['free_memory_gb']:.2f} GB)ï¼Œå¯èƒ½å½±å“æ€§èƒ½"
                )
            
            # æ£€æŸ¥è®¡ç®—èƒ½åŠ›
            compute_capability = float(device_info['compute_capability'])
            if compute_capability < 6.0:
                recommendations['warnings'].append(
                    f"GPU {best_device} è®¡ç®—èƒ½åŠ›è¾ƒä½ ({device_info['compute_capability']})ï¼Œå»ºè®®ä½¿ç”¨æ›´æ–°çš„GPU"
                )
            
            recommendations['recommendations'].extend([
                f"å»ºè®®ä½¿ç”¨GPU {best_device}: {device_info['name']}",
                f"å¯ç”¨å†…å­˜: {device_info['free_memory_gb']:.2f} GB",
                "è®¾ç½®ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES æŒ‡å®šGPUè®¾å¤‡",
                "ä½¿ç”¨æ‰¹å¤„ç†æ–¹å¼å¤„ç†å¤šä¸ªæ–‡æœ¬ä»¥æé«˜æ•ˆç‡"
            ])
            
            # è®¾ç½®æœ€ä¼˜è®¾å¤‡
            if self.set_device(best_device):
                recommendations['recommendations'].append(
                    f"å·²è‡ªåŠ¨åˆ‡æ¢åˆ°æœ€ä¼˜GPUè®¾å¤‡ {best_device}"
                )
        
        return recommendations
    
    def get_batch_size_recommendation(self, model_type: str = "benepar") -> int:
        """æ ¹æ®GPUå†…å­˜æ¨èæ‰¹å¤„ç†å¤§å°"""
        if not self.gpu_available:
            return 8  # CPUé»˜è®¤æ‰¹å¤„ç†å¤§å°
        
        status = self.get_gpu_status()
        current_device_info = None
        
        for device in status['devices']:
            if device['device_id'] == self.current_device:
                current_device_info = device
                break
        
        if not current_device_info:
            return 16  # é»˜è®¤å€¼
        
        free_memory_gb = current_device_info['free_memory_gb']
        
        # æ ¹æ®å¯ç”¨å†…å­˜æ¨èæ‰¹å¤„ç†å¤§å°
        if model_type == "benepar":
            if free_memory_gb >= 8:
                return 64
            elif free_memory_gb >= 4:
                return 32
            elif free_memory_gb >= 2:
                return 16
            else:
                return 8
        
        return 16  # é»˜è®¤å€¼
    
    def clear_cache(self):
        """æ¸…ç†GPUç¼“å­˜"""
        if self.gpu_available:
            try:
                torch.cuda.empty_cache()
                logger.info("GPUç¼“å­˜å·²æ¸…ç†")
            except Exception as e:
                logger.error(f"æ¸…ç†GPUç¼“å­˜å¤±è´¥: {e}")


def check_gpu_environment() -> GPUManager:
    """æ£€æŸ¥GPUç¯å¢ƒå¹¶è¿”å›GPUç®¡ç†å™¨"""
    gpu_manager = GPUManager()
    gpu_manager.print_gpu_status()
    return gpu_manager


def setup_optimal_gpu_environment() -> Tuple[GPUManager, Dict]:
    """è®¾ç½®æœ€ä¼˜GPUç¯å¢ƒ"""
    gpu_manager = GPUManager()
    recommendations = gpu_manager.optimize_for_benepar()
    
    print("\n" + "="*60)
    print("ğŸš€ GPUä¼˜åŒ–å»ºè®®")
    print("="*60)
    
    if recommendations['warnings']:
        print("âš ï¸  è­¦å‘Š:")
        for warning in recommendations['warnings']:
            print(f"   â€¢ {warning}")
        print()
    
    if recommendations['recommendations']:
        print("ğŸ’¡ å»ºè®®:")
        for rec in recommendations['recommendations']:
            print(f"   â€¢ {rec}")
    
    print("="*60)
    
    return gpu_manager, recommendations


if __name__ == "__main__":
    # æµ‹è¯•GPUå·¥å…·
    gpu_manager, recommendations = setup_optimal_gpu_environment()
    
    print(f"\næ¨èæ‰¹å¤„ç†å¤§å°: {gpu_manager.get_batch_size_recommendation()}")